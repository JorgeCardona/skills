{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> HADOOP STACK\n",
    "<img src=\"HadoopStack.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> **QUE ES HADOOP** </center>\n",
    "\n",
    "# Es un framework para **Almacenamiento y Procesamiento de lotes con altos volumenes de datos** de manera distribuida y ejecucion de tareas en paralelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> **COMPONENTES HADOOP** </center>\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"HadoopComponentes.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> **HADOOP COMMON** </center>\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"commons.png\">\n",
    "</div>\n",
    "\n",
    "## - Contiene los archivos y scripts Java Archive (JAR) necesarios para iniciar Hadoop.\n",
    "## - Colección de utilidades y bibliotecas comunes que son necesarias para los otros módulos de Hadoop.\n",
    "## - Proporciona código fuente y documentación. \n",
    "## - Cuenta con una sección de contribuciones que contienen multiples proyectos de la comunidad de Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> **HADOOP YARN** </center>\n",
    "\n",
    "## **YARN** funciona a través de un **Resource Manager** que es uno por nodo y un **Node Manager** que se ejecuta en todos los nodos.\n",
    "\n",
    "* **Resource Manager** este **corre en el master node**. Administra los recursos utilizados en el clúster, **provee un endpoint a los clientes para hacer la solicitud de envio de Jobs** Hace seguimiento del heart beats del Node Manager.\n",
    "    * **Scheduler**  este **decide cómo se asignan los recursos a varias tareas.** Programa los recursos a las **aplicaciones en ejecución en función de los requisitos, la capacidad y la cola**. **No garantiza reiniciar las tareas fallidas**.\n",
    "    * **Application Manager** administra la ejecución de **Application Master en un clúster** y, en caso de falla del contenedor de Application Master, **ayuda a reiniciarlo**. Asimismo, tiene la responsabilidad de aceptar los Jobs enviados.\n",
    "\n",
    "   \n",
    "* **Node Manager** este **corre en el slave node**, Monitorea los **Containers** y Crea **Containers** basado en los requerimientos de la Task, es Responsable de la **ejecución de las Tasks en cada DataNode**. envia **heart beats al Resource Manager**\n",
    "\n",
    "* **Containers** Conjunto de recursos como **RAM, CPU, GPU, Disco, Red** de un Nodo o **Host del Cluster**. Son programados por **Resource Manager** y monitoreados por **Node Manager**.\n",
    "\n",
    "* **Application Master** Es **creado por un Job** y controla la monitorización y la **ejecución de las tareas usando el Container**, es el que **Negocia los recursos para el Job con el Resource Manager**. Gestiona el ciclo de vida de los **Jobs que se ejecutan en el clúster hasta que se completen**.\n",
    "\n",
    "* **History Server** es el encargado de **Almacenar el historial de todos los Jobs**.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"yarn.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> HDFS **HADOOP DISTRIBUTE FILE SYSTEM** Y HCFS **HADOOP COMPATIBLE FILE SYSTEM** </center>\n",
    "# Ss un sistema de archivos distribuido que proporciona espacio de almacenamiento redundante para archivos de gran tamaño especialmente en el rango de terabytes a petabytes.\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"hdfs.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> ARCHIVOS</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Los Archivos que se guardan en el **HDFS son inmutables** y se pueden leer tantas veces como se quiera, por la inmutabilidad no se pueden modificar los archivos.\n",
    "- Los archivos que se van a almacenar en el HDFS **se recomienda que sean de 128 Megas** o mas.\n",
    "\n",
    "- Muchos **Archivos pequeños hacen ineficientes el procesamiento** y el almacenamiento, hay varias razones por las cuales se generan archivos de bajo peso como:\n",
    "    * Los archivos pueden ser la pieza de un archivo lógico más grande. \n",
    "    * Algunos archivos no se pueden combinar en un archivo más grande y son esencialmente pequeños. aunque se comprima un set de imagenes en un archivo que ocupe las 128 megas recomendadas cuando se procesen estas imagenes, se hara de manera separada imagen por imagen, ya que donde cada imagen es un archivo distinto, haciendo ineficiente el proceso.\n",
    "    \n",
    "\n",
    "- **Efectos de los archivos pequeños**: \n",
    "    - HDFS no puede manejar de manera eficiente el almacenamiento de muchos archivos pequeños que son extremadamente pequeños que el tamaño del bloque. \n",
    "    - La lectura de archivos pequeños implica muchas búsquedas y muchos saltos entre nodos de datos, lo que a su vez es un procesamiento de datos ineficiente.\n",
    "    - En la memoria de namenode, cada archivo, directorio y bloque en HDFS se representa como un objeto. Cada uno de estos objetos tiene un tamaño de 150 bytes.\n",
    "    - Si consideramos 10 millones de archivos pequeños, cada uno de estos archivos usará un bloque separado. Eso provocará un uso de 3 gigas de memoria.\n",
    "    - Con las limitaciones de hardware que tiene, escalar más allá de este nivel es un problema. \n",
    "    - Con muchos archivos, la memoria requerida para almacenar los metadatos es alta y no se puede escalar más allá de un límite.\n",
    "    - En MapReduce, la tarea Map procesa un bloque de datos a la vez. \n",
    "    - Muchos archivos pequeños significan muchos bloques, lo que significa muchas tareas y mucha contabilidad por parte de Application Master.\n",
    "    - Esto ralentizará el rendimiento general del clúster en comparación con el procesamiento de archivos grandes.\n",
    "\n",
    "    **Alternativas**:\n",
    "\n",
    "    **1\\. Archivos Hadoop, HAR**:\n",
    "     - Hadoop Archives o HAR es una instalación de archivo que empaqueta archivos en bloques HDFS de manera eficiente. No relacione los archivos .har con archivos comprimidos.\n",
    "     - HAR se crea a partir de una colección de archivos y la herramienta de archivo (un comando simple) ejecutará un trabajo de MapReduce para procesar los archivos de entrada en paralelo y crear un archivo de archivos.\n",
    "     - los pequeños archivos dentro del archivo .har serán procesados individualmente por mapeadores separados, lo cual es ineficiente.\n",
    "\n",
    "    **2\\. SequenceFile System**:\n",
    "     * Use el nombre de cada archivo como clave y el contenido del cada archivo como valor.\n",
    "     * Puede ser procesados en forma de Streaming (directamente o usando MapReduce) operando en SequenceFile\n",
    "     * Se pueden dividir, por lo que MapReduce puede dividirlos en fragmentos y operar en cada fragmento de forma independiente.\n",
    "     * Admiten compresión, a diferencia de los HAR. La compresión de bloques es la mejor opción en la mayoría de los casos, ya que comprime bloques de varios registros (en lugar de por registro).\n",
    "\n",
    "    **3\\. HBase si no es un problema la latencia**:\n",
    "     - Si está produciendo muchos archivos pequeños, entonces, dependiendo del patrón de acceso, un tipo diferente de almacenamiento podría ser más apropiado.\n",
    "     - Almacena datos en MapFiles (SequenceFiles indexados) y es una buena opción si necesita realizar análisis Streaming de estilo MapReduce con la búsqueda aleatoria ocasional.\n",
    "     \n",
    "* **Secundary Namenode** Asume la responsabilidad de fusionar los registros de edición con fsimage del namenode. Es Como un LogShipping cronometrado.\n",
    "* **StandBy Namenode** Releva al namenode cuando este falla. Es como Streaming Replication con failover integrado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> TIPOS DE ARCHIVOS</center>\n",
    "\n",
    "- Sequential files\n",
    "- Text\n",
    "- Avro : sistema de serialización de datos, en un formato binario compacto independiente del Lenguaje. Usa esquemas basados en JSON.  utiliza llamadas RPC para enviar datos, durante el intercambio de datos, se envia el esquema. \n",
    "- ORC\n",
    "- Parquet\n",
    "- Arrow\n",
    "- Protocol Buffers\n",
    "- Thrift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> ALGORITMOS DE COMPRESION</center>\n",
    "- Snappy\n",
    "- Zlib\n",
    "- Gzip\n",
    "- LZO\n",
    "- Bzip2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"cluster.png\">\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"HadoopVentajas.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> **MAP-REDUCE**</center>\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"mapreduce.png\">\n",
    "</div>\n",
    "- Secondary Sort : Ordena los valores de la fase de Mapping antes de entregarsela al reductor de manera ascendente o descendente, en los casos que sean necesarios por ejemplo para una serie de tiempo.\n",
    "- Join: Son usados para unir dos o más conjuntos de datos en función de alguna columna.\n",
    "    - Map Side Join:\n",
    "        - la operación de Join se realiza en la fase Map.\n",
    "        - Se usa cuando un conjunto de datos es grande y el otro conjunto de datos es pequeño.\n",
    "        - Ordenado por la misma clave.\n",
    "        - Igual número de particiones.\n",
    "        - Todos los registros de la misma clave deben estar en la misma partición.\n",
    "    - Reduce Side Join:\n",
    "        - la operación de Join se realiza en la fase Reduce.\n",
    "        - Se usa cuando ambos conjunto de datos son grandes.\n",
    "        - Mucho más flexible de implementar.\n",
    "        - Tiene que haber WritableComparable personalizado con la función necesaria anulada.\n",
    "        - Necesitamos un particionador personalizado.\n",
    "        - Se requiere un comparador de grupo personalizado.\n",
    "    \n",
    "**Si tiene experiencia en SQL, no necesita preocuparse por escribir el código MapReduce Java para realizar una operación de unión. Puede utilizar Hive como alternativa.**\n",
    "    \n",
    "- Chained Mappers: permite usar múltiples clases de Mapper dentro de una sola tarea de Map.\n",
    "- Chained Reducer permite encadenar varias clases de Mapper después de un Reducer dentro de la tarea Reducer.\n",
    "- Bloom Filter\n",
    "- In memory Mapper/Reducer side join\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>FASES DEL PARADIGMA **MAP-REDUCE**\n",
    "<img src=\"MapReduce-fases.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> DEFINE LOS DATOS CON QUE SE VA A GENERAR LA INFORMACION DE ENTRADA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uYZHdogp2r8I"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "valores = ['CERO','UNO','DOS','TRES','CUATRO','CINCO','SEIS','SIETE','OCHO','NUEVE','DIEZ','ZERO','ONE','TWO','THREE','FOUR','FIVE','SIX','SEVEN','EIGHT','NINE','TEN']\n",
    "valores = ['CERO','UNO','DOS','TRES','CUATRO','CINCO','SEIS','SIETE','OCHO','NUEVE','DIEZ']\n",
    "\n",
    "caracter_separador = ' @'\n",
    "valores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> GENERA LOS DATOS A PROCESAR **INPUT**\n",
    "<img src=\"MapReduce-fases.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "NT1Asq641kvM",
    "outputId": "4949f02d-e5fe-45b6-be49-6a6ab38e948c",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# retorna una lista con valores\n",
    "def generar_valores(limite_inferior:int = 2, limite_superior:int = 10,caracter_separador:str = caracter_separador) -> list:\n",
    "    return [random.choice(valores) for valor in range(random.randint(limite_inferior, limite_superior))]\n",
    "\n",
    "# retorna una cadena de valores con la informacion a particionar\n",
    "def generar_archivo_de_datos(lista_valores_entrada:list, cantidad_de_muestras:int = 100)-> str:\n",
    "\n",
    "  # recupera cada lista generada la convierte a cadena y le adiciona un caracter para usarlo como separador\n",
    "  texto = [' '.join(generar_valores()) + caracter_separador for limite_listas in range(cantidad_de_muestras)]\n",
    "\n",
    "  # une el contenido de las listas en una sola cadena\n",
    "  return ''.join(texto)\n",
    "\n",
    "archivo_de_datos = generar_archivo_de_datos(generar_valores())\n",
    "archivo_de_datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> DIVIDE LOS DATOS A PROCESAR BASADO EN EL CARACTER SEPARADOR **@** **SPLITTING**\n",
    "<img src=\"MapReduce-fases.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YaTXah2Z7gc1",
    "outputId": "60962140-2e94-406c-d89b-b2afeee9984d",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# divide el archivo en listas\n",
    "def splitting(arhivo_de_datos) -> list:\n",
    "  # se pone [:-1] para que no genere una cadena vacia al final\n",
    "  return arhivo_de_datos.split(caracter_separador)[:-1]\n",
    "\n",
    "splitting_text = splitting(archivo_de_datos)\n",
    "splitting_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> CREA LA INFORMACION DE LOS REGISTROS DEL **RECORDREADER** PARA QUE SEAN USADOS POR EL **MAPPER**\n",
    "<img src=\"MapReduce-fases.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def record_reader(input_file):\n",
    "    record_file = []\n",
    "    inicio = 0\n",
    "    for record in input_file:\n",
    "        \n",
    "        record_file.append((inicio, record))\n",
    "        inicio += len(record)\n",
    "    \n",
    "    return record_file\n",
    "        \n",
    "record_reader_file = record_reader(splitting_text)\n",
    "record_reader_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> EJECUTA EL CODIGO DE MAPEO DE LOS DATOS **MAPPING**\n",
    "<img src=\"MapReduce-fases.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0mQvjndG7Ran",
    "outputId": "426b7fe5-177a-4e7d-a213-0145ad525d34",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lista_a_lista_de_diccionarios(lista):\n",
    "    return [{str(llave):1} for llave in lista]\n",
    "\n",
    "# divide cada palabra 1 a 1 en cada splitting text\n",
    "def mapping(record_reader_file):\n",
    "    mapeo = []\n",
    "    \n",
    "    for record_reader in record_reader_file:\n",
    "        lista_cadena = record_reader[1].split()\n",
    "        mapeo.append(lista_a_lista_de_diccionarios(lista_cadena))\n",
    "        \n",
    "    return mapeo\n",
    "\n",
    "lista_mapping = mapping(record_reader_file)\n",
    "print(lista_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> COMBINA LOS DATOS DE LAS CLAVES DEL MISMO BLOQUE DE MAPEO **COMBINER**\n",
    "<img src=\"MapReduce-fases.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7fymcYSka87T",
    "outputId": "9f86f215-1a9a-4e01-be43-71c27d4ebd58",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unir_claves(lista):\n",
    "    return {list(clave.keys())[0]:lista.count(clave) for clave in lista}\n",
    "\n",
    "# junta las claves (las une) de cada particion del mapper\n",
    "def combiner(lista_mapping):\n",
    "  \n",
    "  return [unir_claves(lista) for lista in lista_mapping]\n",
    "\n",
    "combiner_values = combiner(lista_mapping)\n",
    "print(combiner_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> CREA PARTICIONES USANDO EN ESTE CASO COMO BASE DE PARTICION LOS NOMBRES DE LAS CLAVES **PARTITIONER**\n",
    "<img src=\"MapReduce-fases.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rH71jXL18IFR",
    "outputId": "056a82ce-9831-4a55-97d8-4d846ae479d9",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# divide cada diccionario de un combiner y genera una lista de diccionarios separados\n",
    "def generar_diccionarios(lista:list):\n",
    "    return [{key:value} for key,value in lista.items()]\n",
    "\n",
    "# genera particiones por cada clave basado en el nombre de la clave\n",
    "def generar_lista_de_diccionarios(combiner_values):\n",
    "    particion = list()\n",
    "    \n",
    "    for particion_valores in combiner_values:\n",
    "        particion.extend(generar_diccionarios(particion_valores))\n",
    "    \n",
    "    return particion\n",
    "  \n",
    "# particionado para este ejemplo por clave del diccionarario\n",
    "def particionar(combiner_values):\n",
    "    valores_lista_de_diccionarios = generar_lista_de_diccionarios(combiner_values)\n",
    "    particion = dict()\n",
    "    for diccionario in valores_lista_de_diccionarios:\n",
    "        llave = list(diccionario.keys())[0]\n",
    "        \n",
    "        if llave in particion:\n",
    "            valor = particion.get(llave)\n",
    "            valor.append(diccionario)\n",
    "            particion[llave] = valor\n",
    "        else:\n",
    "            particion[llave] = [diccionario]\n",
    "            \n",
    "    return particion\n",
    "\n",
    "claves_particionadas = particionar(combiner_values)\n",
    "print(claves_particionadas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> AGRUPA LOS DATOS Y LOS ORDENA CREANDO UNA LISTA CON CADA VALOR PARA CADA CLAVE **SHUFFLE AND SORT**\n",
    "<img src=\"MapReduce-fases.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fSaeEc0g8YhZ",
    "outputId": "2006c288-e3db-499a-f033-ad38ac8ac267",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def shuffle_and_short(claves_particionadas):\n",
    "    \n",
    "    particiones_agrupadas = dict()\n",
    "    \n",
    "    for llave in claves_particionadas:\n",
    "        particiones_agrupadas[llave] = [list(valor.values())[0] for valor in claves_particionadas.get(llave)]\n",
    "        \n",
    "    return particiones_agrupadas\n",
    "  \n",
    "\n",
    "shuffle_and_short_ordenados = shuffle_and_short(claves_particionadas)\n",
    "\n",
    "print(shuffle_and_short_ordenados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> EJECUTA EL ALGORITMO DE REDUCCION PARA CADA CLAVE, EN ESTE CASO LOS SUMA **REDUCER**\n",
    "<img src=\"MapReduce-fases.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7XHsVDjF8vzF",
    "outputId": "ca41887d-32a0-4243-83e8-088eb40b24d9"
   },
   "outputs": [],
   "source": [
    "def reducer(shuffle_and_short_ordenados):\n",
    "    valores_reducidos = dict()\n",
    "    \n",
    "    for llave in shuffle_and_short_ordenados:\n",
    "        \n",
    "        valores_reducidos[llave] = sum(shuffle_and_short_ordenados.get(llave))\n",
    "        \n",
    "    return valores_reducidos\n",
    "\n",
    "datos_reducer = reducer(shuffle_and_short_ordenados)\n",
    "datos_reducer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> GENERA EL ARCHIVO DE SALIDA CON LOS DATOS FINALES **OUTPUT**\n",
    "<img src=\"MapReduce-fases.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "id": "pVHfCVF7ALfP",
    "outputId": "d746763b-3004-45c5-9e21-65995a9f877f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def output_dataframe(datos_reducer):\n",
    "\n",
    "  # crea los valores de las keys en tipo lista para que se puedan adicionar al dataframe sin problemas\n",
    "  datos_pandas = {key:[value] for key, value in datos_reducer.items()}\n",
    "  \n",
    "  return pd.DataFrame(datos_pandas)\n",
    "\n",
    "output_dataframe(datos_reducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Hadoop-MapReduce.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
