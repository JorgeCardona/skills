{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción al aprendizaje profundo (Deep learning) con PyTorch\n",
    "\n",
    "<p style='text-align: justify;'> En este notebook, se le presentará \n",
    "<a href=\"http://pytorch.org/\" title=\"PyTorch\">PyTorch</a>\n",
    ", un framework para construir y entrenar redes neuronales. PyTorch en muchos sentidos se comporta como los array y matrices de Numpy. Estos arrays y matrices Numpy, después de todo, son solo \n",
    "<a href=\"https://towardsdatascience.com/quick-ml-concepts-tensors-eb1330d7760f\" title=\"Tensores\">Tensores</a>. \n",
    "PyTorch toma estos tensores y simplifica el envio de datos a las GPU para el procesamiento más rápido necesario al entrenar redes neuronales. También proporciona un módulo que calcula automáticamente los gradientes (¡para backpropagation!) Y otro módulo específicamente para construir redes neuronales (Neural Networks). En conjunto, PyTorch termina siendo más afin con Python y Numpy/Scipy stack en comparación con TensorFlow y otros frameworks. </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes Neuronales - Neural Networks\n",
    "\n",
    "<p style='text-align: justify;'> \n",
    "El aprendizaje profundo (Deep Learning) se basa en redes neuronales artificiales que han existido de alguna forma desde finales de la década de 1950. Las redes se construyen a partir de partes individuales que se aproximan a las neuronas, normalmente llamadas unidades o simplemente \"neuronas\". Cada unidad tiene una cierta cantidad de entradas ponderadas. Estas entradas ponderadas se suman (una combinación lineal) y luego se pasan a través de una función de activación para obtener la salida de la unidad.\n",
    "</p>\n",
    "\n",
    "\n",
    "<img src=\"assets/simple_neuron.png\" width=400px>\n",
    "\n",
    "Matemáticamente esto se ve así:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= f(w_1 x_1 + w_2 x_2 + b) \\\\\n",
    "y &= f\\left(\\sum_i w_i x_i +b \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "Con vectores, este es el producto-punto interno de dos vectores:\n",
    "\n",
    "$$\n",
    "h = \\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots  x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_1 \\\\\n",
    "           w_2 \\\\\n",
    "           \\vdots \\\\\n",
    "           w_n\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "<p style='text-align: justify;'> \n",
    "Los cálculos de redes neuronales son solo un montón de operaciones de álgebra lineal en *tensores*, una generalización de matrices. Un vector es un tensor unidimensional, una matriz es un tensor bidimensional, una matriz con tres índices es un tensor tridimensional (imágenes en color RGB, por ejemplo). La estructura de datos fundamental para las redes neuronales son los tensores y PyTorch (así como casi todos los demás marcos de aprendizaje profundo) se basa en tensores.\n",
    "</p>\n",
    "\n",
    "<img src=\"assets/tensor_examples.svg\" width=600px>\n",
    "\n",
    "Con los conceptos básicos cubiertos, es hora de explorar cómo podemos usar PyTorch para construir una red neuronal simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primero, importamos PyTorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos la funcion de activacion\n",
    "def activation(x):\n",
    "    \"\"\" Sigmoid activation function \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### creamos algunos datos\n",
    "torch.manual_seed(7) # Asignamos un valor aleatorio para que el analosis se torne predecible\n",
    "\n",
    "# Features son 5 variables aleatorias normales\n",
    "features = torch.randn((1, 5))\n",
    "# pesos verdaderos para los datos usando de nuevo variables aleatorias\n",
    "weights = torch.randn_like(features)\n",
    "# se defina el sesgo\n",
    "bias = torch.randn((1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1468,  0.7861,  0.9468, -1.1143,  1.6908]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arriba generamos datos que podemos usar para obtener el resultado de nuestra red simple. Todo esto es simplemente aleatorio por ahora, en el futuro comenzaremos a usar datos normales. Pasando por cada línea relevante:\n",
    "\n",
    "\n",
    "`features = torch.randn((1, 5))` creamos un tensor con forma `(1, 5)`, una fila y cinco columnas, que contiene valores distribuidos aleatoriamente de acuerdo con la distribución normal con una media de cero y una desviación estándar de uno. \n",
    "\n",
    "`weights = torch.randn_like(features)` creamos otro tensor con la misma forma que `features`, nuevamente conteniendo valores de una distribución normal.\n",
    "\n",
    "por ultimo, `bias = torch.randn((1, 1))` crea un valor único a partir de una distribución normal.\n",
    "\n",
    "Los tensores de PyTorch se pueden sumar, multiplicar, restar, etc., al igual que las matrices Numpy. En general, usaremos tensores PyTorch de la misma manera que usariamos matrices en Numpy. Sin embargo, vienen con algunos beneficios interesantes, como la aceleración de la GPU, que veremos más adelante. Por ahora, utilicemos los datos generados para calcular la salida de esta simple red de una sola capa.\n",
    "\n",
    "> ** Ejercicio **: Calcule la salida de la red con las características de entrada \"features\", ponderaciones \"weights\" y sesgo \"bias\". Similar a Numpy, PyTorch tiene un [`torch.sum()`](https://pytorch.org/docs/stable/torch.html#torch.sum) función, así como un método `.sum ()` sobre tensores, para tomar sumas. Utilice la función \"activación\" definida anteriormente como función de activación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1626)\n",
      "tensor(-1.5621)\n",
      "tensor(0.3177)\n",
      "\n",
      "tensor(0.8968)\n",
      "tensor(0.1733)\n",
      "tensor(0.5788)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Calculemos la salida de esta red usando las caracteristicas(features), los pesos(weights) y el el sesgo(bias) del tensor.\n",
    "\n",
    "print(torch.sum(features))\n",
    "print(torch.sum(weights))\n",
    "print(torch.sum(bias))\n",
    "print()\n",
    "print(activation(features.sum()))\n",
    "print(activation(weights.sum()))\n",
    "print(activation(bias.sum()))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solucion\n",
    "y = activation(torch.sum(features * weights) + bias)\n",
    "y = activation((features * weights).sum() + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solution\n",
    "\n",
    "y = activation(torch.mm(features, weights.view(5,1)) + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos hacer la multiplicación y la suma en la misma operación usando una multiplicación de matrices. En general, se busca usar multiplicaciones de matrices, ya que son más eficientes y aceleradas al usar bibliotecas modernas y computación de alto rendimiento en GPU.\n",
    "\n",
    "Aquí, queremos hacer una multiplicación matricial de las características y los pesos. Para esto podemos usar [`torch.mm()`](https://pytorch.org/docs/stable/torch.html#torch.mm) or [`torch.matmul()`](https://pytorch.org/docs/stable/torch.html#torch.matmul) que es algo más complicado y soporta la radiodifusión. Si nosotros intentamos hacerlo con `features` y` weights` tal como están, obtendremos un error\n",
    "```python\n",
    ">> torch.mm(features, weights)\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "RuntimeError                              Traceback (most recent call last)\n",
    "<ipython-input-13-15d592eb5279> in <module>()\n",
    "----> 1 torch.mm(features, weights)\n",
    "\n",
    "RuntimeError: size mismatch, m1: [1 x 5], m2: [1 x 5] at /Users/soumith/minicondabuild3/conda-bld/pytorch_1524590658547/work/aten/src/TH/generic/THTensorMath.c:2033\n",
    "```\n",
    "\n",
    "A medida que construye redes neuronales en cualquier marco, verá esto a menudo. Muy a menudo. Lo que sucede aquí es que nuestros tensores no tienen las formas correctas para realizar una multiplicación de matrices. Recuerde que para las multiplicaciones de matrices, el número de columnas en el primer tensor debe ser igual al número de filas en la segunda columna. Tanto las \"características\" como los \"pesos\" tienen la misma forma, \"(1, 5)\". Esto significa que debemos cambiar la forma de los \"pesos\" para que funcione la multiplicación de matrices.\n",
    "\n",
    "** Nota: ** Para ver la forma de un tensor llamado `tensor`, usa` tensor.shape`. Si está creando redes neuronales, utilizará este método con frecuencia.\n",
    "\n",
    "Aquí hay algunas opciones: [`weights.reshape()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.reshape), [`weights.resize_()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.resize_), y [`weights.view()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view).\n",
    "\n",
    "* `weights.reshape(a, b)` devolverá un nuevo tensor con los mismos datos que \"pesos\" con tamaño \"(a, b)\" a veces, ya veces un clon, ya que copia los datos a otra parte de la memoria.\n",
    "* `weights.resize_(a, b)` devuelve el mismo tensor con una forma diferente. Sin embargo, si la nueva forma da como resultado menos elementos que el tensor original, algunos elementos se eliminarán del tensor (pero no de la memoria). Si la nueva forma da como resultado más elementos que el tensor original, los elementos nuevos no se inicializarán en la memoria. Aquí debo señalar que el guión bajo al final del método denota que este método se realiza ** en el lugar **. Aquí hay un gran hilo del foro de [read more about in-place operations](https://discuss.pytorch.org/t/what-is-in-place-operation/16244) en PyTorch.\n",
    "* `weights.view(a, b)` devolverá un nuevo tensor con los mismos datos que \"weights\" con tamaño \"(a, b)\".\n",
    "\n",
    "Normalmente ese `.view()`, pero cualquiera de los tres métodos funcionará para esto. Entonces, ahora podemos cambiar la forma de \"weights\" para tener cinco filas y una columna con algo como \"weights.view(5, 1)\".\n",
    "\n",
    "> ** Ejercicio **: Calcula la salida de nuestra pequeña red usando la multiplicación de matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1595]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Calculate the output of this network using matrix multiplication\n",
    "features\n",
    "weights = weights.view(5,1)\n",
    "activation(torch.mm(features, weights) + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack them up!\n",
    "\n",
    "Así es como se puede calcular la salida de una sola neurona. El verdadero poder de este algoritmo ocurre cuando comienzas a apilar estas unidades individuales en capas y pilas de capas, en una red de neuronas. La salida de una capa de neuronas se convierte en la entrada para la siguiente capa. Con múltiples unidades de entrada y unidades de salida, ahora necesitamos expresar los pesos como una matriz.\n",
    "\n",
    "<img src='assets/multilayer_diagram_weights.png' width=450px>\n",
    "\n",
    "La primera capa que se muestra en la parte inferior aquí son las entradas, se conoce como ** capa de entrada **. La capa intermedia se llama ** capa oculta ** y la capa final (a la derecha) es la ** capa de salida **. Podemos expresar esta red matemáticamente con matrices nuevamente y usar la multiplicación de matrices para obtener combinaciones lineales para cada unidad en una operación. Por ejemplo, la capa oculta ($h_1 $ y $h_2 $ aquí) se puede calcular\n",
    "\n",
    "$$\n",
    "\\vec{h} = [h_1 \\, h_2] = \n",
    "\\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots \\, x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_{11} & w_{12} \\\\\n",
    "           w_{21} &w_{22} \\\\\n",
    "           \\vdots &\\vdots \\\\\n",
    "           w_{n1} &w_{n2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "La salida para esta pequeña red se encuentra tratando la capa oculta como entradas para la unidad de salida. La salida de la red se expresa simplemente\n",
    "\n",
    "$$\n",
    "y =  f_2 \\! \\left(\\, f_1 \\! \\left(\\vec{x} \\, \\mathbf{W_1}\\right) \\mathbf{W_2} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generamos datos\n",
    "torch.manual_seed(7) # Asignamos un valor aleatorio para que el analosis se torne predecible\n",
    "\n",
    "# Features son 3 variables aleatorias normales\n",
    "features = torch.randn((1, 3))\n",
    "\n",
    "# Define los valores de cada capa in la red\n",
    "n_input = features.shape[1]     # Number of input units, must match number of input features\n",
    "n_hidden = 2                    # Number of hidden units \n",
    "n_output = 1                    # Number of output units\n",
    "\n",
    "# Pesos para las entradas de la capa oculta\n",
    "W1 = torch.randn(n_input, n_hidden)\n",
    "# Pesos de la capa oculta a la capa de salida\n",
    "W2 = torch.randn(n_hidden, n_output)\n",
    "\n",
    "# definimos los términos de sesgo para capas ocultas y de salida\n",
    "B1 = torch.randn((1, n_hidden))\n",
    "B2 = torch.randn((1, n_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Calculate the output for this multi-layer network using the weights `W1` & `W2`, and the biases, `B1` & `B2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3171]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Your solution here\n",
    "#calculate firts hidden layer\n",
    "x = activation(torch.mm(features, W1) + B1)\n",
    "#calculate lastone hidden layer\n",
    "activation(torch.mm(x,W2) + B2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si hizo esto correctamente, debería ver la salida `tensor ([[0.3171]])`.\n",
    "\n",
    "El número de unidades ocultas es un parámetro de la red, a menudo llamado ** hiperparámetro ** para diferenciarlo de los parámetros de ponderaciones y sesgos. Como verá más adelante, cuando analicemos el entrenamiento de una red neuronal, cuantas más unidades ocultas tenga una red y más capas, mejor podrá aprender de los datos y hacer predicciones precisas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy to Torch and back\n",
    "\n",
    "Sección de bonificación especial! PyTorch tiene una gran característica para convertir entre matrices Numpy y tensores Torch. Para crear un tensor a partir de una matriz Numpy, use `torch.from_numpy()`. Para convertir un tensor en una matriz Numpy, use el método `.numpy ()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.67074982,  0.90466332,  0.46128531],\n",
       "       [ 0.51230911,  0.18835729,  0.43471674],\n",
       "       [ 0.22338983,  0.76382028,  0.67405823],\n",
       "       [ 0.82932055,  0.65622593,  0.0371518 ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.rand(4,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6707,  0.9047,  0.4613],\n",
       "        [ 0.5123,  0.1884,  0.4347],\n",
       "        [ 0.2234,  0.7638,  0.6741],\n",
       "        [ 0.8293,  0.6562,  0.0372]], dtype=torch.float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.67074982,  0.90466332,  0.46128531],\n",
       "       [ 0.51230911,  0.18835729,  0.43471674],\n",
       "       [ 0.22338983,  0.76382028,  0.67405823],\n",
       "       [ 0.82932055,  0.65622593,  0.0371518 ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La memoria se comparte entre la matriz Numpy y el tensor Torch, por lo que si cambia los valores en el lugar de un objeto, el otro también cambiará."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.6830,  3.6187,  1.8451],\n",
       "        [ 2.0492,  0.7534,  1.7389],\n",
       "        [ 0.8936,  3.0553,  2.6962],\n",
       "        [ 3.3173,  2.6249,  0.1486]], dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiply PyTorch Tensor by 2, in place\n",
    "b.mul_(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.6829993 ,  3.61865328,  1.84514124],\n",
       "       [ 2.04923646,  0.75342916,  1.73886697],\n",
       "       [ 0.89355934,  3.05528112,  2.69623291],\n",
       "       [ 3.31728218,  2.62490371,  0.1486072 ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numpy array matches new values from Tensor\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
